{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u> Movie Recommender system : Implicit Ratings </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The aim of this project is to evaluate the performance of user-user Collaborative Filtering, Matrix Factorization and Bayesian Personalized Ranking from scratch and to observe how the results are varying from a naive popularity based recommender system. As opposed to Explicit rating where our aim was to predict the rating, here our aim is to recommend movies to the user.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Implicit Ratings*\n",
    "\n",
    "Implicit ratings are a type of rating system that are derived from indirect or implicit feedback, rather than explicit ratings or reviews provided by users. Implicit ratings are inferred from a user's behavior or actions, such as the amount of time spent on a particular page, the frequency of clicks or views, or the type of content accessed.\n",
    "\n",
    "For example, an online retailer may track a customer's browsing and purchasing history to infer their preferences and provide product recommendations based on those implicit ratings. Similarly, a music streaming service may use implicit ratings to create personalized playlists based on a user's listening habits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dataset*\n",
    "The MovieLens dataset is a popular benchmark dataset in the field of recommender systems. It contains a large number of movie ratings provided by users, along with movie metadata such as title, genre, and release year. Majorly, the dataset contains three files </br>\n",
    "* Movie data - which contains details of the movie like Genre, title, release year etc.\n",
    "* Rating data - which contains the ratings provided by users to each movie. Although the ratings in the dataset are from 0-5, we will map it to a binary rating scale to evaluate implicit recommendation.\n",
    "* User data - which contains the demographics of users\n",
    "</br> </br>\n",
    "For the purpose of this project, we will focus on the rating file and aim to predict the ratings (binary) assigned by each user to individual movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Algorithms*\n",
    "\n",
    "Here we will try out the below 3 algorithms and compare their performance through selected evaluation metrics: </br>\n",
    "* Global average - Most naive form of predicting ratings without taking users into consideration. Here, each user will have the same ratings for a movie which is the average rating of the movie across all users. </br></br>\n",
    "* User-user collaborative filtering - Here we will find users who have similar tastes and preferences to the user for whom recommendations are being made. Once similar users are identified, a weighted average of their ratings and preferences for movies are used to make recommendations for the target user.</br></br>\n",
    "* Matrix Factorization -  This algorithm involves breaking down the user-item rating matrix into two lower-dimensional matrices: one representing the users and the other representing the items. These matrices are designed to capture the underlying characteristics of users and items that contribute to their preferences and ratings. These matrices can be used to predict the ratings for new items and make recommendations for users by computing the dot product of the user and item vectors.</br></br>\n",
    "* Bayesian Personalized Ranking (BPR) - BPR uses a probabilistic model to estimate the likelihood that a user will prefer an item based on their past interactions with the system. Specifically, BPR models the probability that a user prefers one item over another using a pairwise ranking approach. This means that instead of predicting a numerical rating for each item, BPR predicts a preference ranking between pairs of items. </br></br> One of the key advantages of BPR is that it can handle large datasets with sparse interactions, where users have only interacted with a small subset of items. BPR also allows for easy incorporation of additional user and item features, such as demographic information or item attributes, to improve the quality of recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Evaluation Metrics*\n",
    "\n",
    "We will consider two metrics for evaluation:\n",
    "* Precision\n",
    "* Recall\n",
    "\n",
    "Each metric will be evaluated for top-K recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import required libraries\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Reading data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file 'ratings.dat' (downloadable from https://grouplens.org/datasets/movielens/1m/) contains data of the format </br>\n",
    "$<UserID> :: <MovieID> :: <Rating> :: <Timestamp>$\n",
    "</br>\n",
    "Let's read and store the data into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZaocWZ94MD-e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Data : 1000209 x 4\n",
      "\n",
      "SAMPLE FROM ACTUAL DATASET \n",
      "--------------------------\n",
      "   UserID  MovieID  Rating  Timestamp\n",
      "0       1     1193       5  978300760\n",
      "1       1      661       3  978302109\n",
      "2       1      914       3  978301968\n",
      "3       1     3408       4  978300275\n",
      "4       1     2355       5  978824291\n",
      "\n",
      "\n",
      "DATA CONVERTED TO USER x MOVIES FORMAT\n",
      "--------------------------------------\n",
      "      0     1     2     3     4     5     6     7     8     9     ...  3696  \\\n",
      "0        0     3     0     4     5     3     0     0     0     4  ...     0   \n",
      "1        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "2        0     0     0     0     5     5     0     0     0     0  ...     0   \n",
      "3        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "4        0     0     0     3     5     0     0     0     0     4  ...     0   \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
      "6035     5     0     3     0     0     5     0     0     5     0  ...     0   \n",
      "6036     4     0     0     0     0     0     0     4     0     4  ...     0   \n",
      "6037     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "6038     0     3     4     0     0     4     0     0     0     4  ...     0   \n",
      "6039     4     0     0     0     0     0     0     0     0     5  ...     0   \n",
      "\n",
      "      3697  3698  3699  3700  3701  3702  3703  3704  3705  \n",
      "0        0     0     0     0     0     0     0     0     0  \n",
      "1        0     0     0     0     0     0     0     0     0  \n",
      "2        0     0     0     0     0     0     0     0     0  \n",
      "3        0     0     0     0     0     0     0     0     0  \n",
      "4        0     0     0     0     0     0     0     0     0  \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "6035     0     0     0     0     0     0     0     0     0  \n",
      "6036     0     0     0     0     0     0     0     0     0  \n",
      "6037     0     0     0     0     0     0     0     0     0  \n",
      "6038     0     0     0     0     0     0     0     0     0  \n",
      "6039     0     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[6040 rows x 3706 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Read data\n",
    "\"\"\"\n",
    "\n",
    "data_df = pd.read_csv('ratings.dat', sep='::',\n",
    "                      names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"],\n",
    "                      engine='python')\n",
    "print(\"Shape of Data : {} x {}\\n\".format(data_df.shape[0], data_df.shape[1]))\n",
    "print(\"SAMPLE FROM ACTUAL DATASET \")\n",
    "print(\"--------------------------\")\n",
    "print(data_df.head())\n",
    "\n",
    "TRAIN_SIZE = 0.7\n",
    "\n",
    "\n",
    "# First, generate dictionaries for mapping old id to new id for users\n",
    "# and movies\n",
    "# unique_MovieID = data_df['MovieID'].unique()\n",
    "unique_UserID = data_df['UserID'].unique()\n",
    "j = 0\n",
    "user_old2new_id_dict = dict()\n",
    "for u in unique_UserID:\n",
    "    user_old2new_id_dict[u] = j\n",
    "    j += 1\n",
    "j = 0\n",
    "movie_old2new_id_dict = dict()\n",
    "for i in unique_MovieID:\n",
    "    movie_old2new_id_dict[i] = j\n",
    "    j += 1\n",
    "\n",
    "# Then, use the generated dictionaries to reindex UserID and MovieID\n",
    "# in the data_df\n",
    "user_list = data_df['UserID'].values\n",
    "movie_list = data_df['MovieID'].values\n",
    "for j in range(len(data_df)):\n",
    "    user_list[j] = user_old2new_id_dict[user_list[j]]\n",
    "    movie_list[j] = movie_old2new_id_dict[movie_list[j]]\n",
    "data_df['UserID'] = user_list\n",
    "data_df['movieID'] = movie_list\n",
    "\n",
    "# Generate train_df with 70% samples and test_df with 30% samples,\n",
    "# and there should have no overlap between them.\n",
    "train_index = np.random.random(len(data_df)) <= TRAIN_SIZE\n",
    "train_df = data_df[train_index]\n",
    "test_df = data_df[~train_index]\n",
    "\n",
    "# Number of Unique Users\n",
    "num_user = len(data_df['UserID'].unique())   \n",
    "# Number of Unique Movies\n",
    "num_movie = len(data_df['MovieID'].unique())\n",
    "\n",
    "# Generate train_mat and test_mat\n",
    "train_mat = coo_matrix(\n",
    "    (train_df['Rating'].values,\n",
    "     (train_df['UserID'].values, train_df['MovieID'].values)),\n",
    "    shape=(num_user, num_movie)\n",
    ").astype(float).toarray()\n",
    "test_mat = coo_matrix(\n",
    "    (test_df['Rating'].values,\n",
    "     (test_df['UserID'].values, test_df['MovieID'].values)),\n",
    "    shape=(num_user, num_movie)\n",
    ").astype(float).toarray()\n",
    "\n",
    "# Visualize the new Matrix : rows -> users, columns -> movies\n",
    "print(\"\\n\\nDATA CONVERTED TO USER x MOVIES FORMAT\")\n",
    "print(\"--------------------------------------\")\n",
    "print(pd.DataFrame(train_mat, dtype = int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-sEqkSHL77V"
   },
   "source": [
    "We will convert explicit recommendations to implicit by replacing all non-zero ratings as 1. The idea is that whenever a user 'rated' the movie, he/she liked it (although it is not always correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Av77NlOLL77V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "DATA CONVERTED TO IMPLICIT RATINGS\n",
      "--------------------------------------\n",
      "      0     1     2     3     4     5     6     7     8     9     ...  3696  \\\n",
      "0        0     1     0     1     1     1     0     0     0     1  ...     0   \n",
      "1        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "2        0     0     0     0     1     1     0     0     0     0  ...     0   \n",
      "3        0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "4        0     0     0     1     1     0     0     0     0     1  ...     0   \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
      "6035     1     0     1     0     0     1     0     0     1     0  ...     0   \n",
      "6036     1     0     0     0     0     0     0     1     0     1  ...     0   \n",
      "6037     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
      "6038     0     1     1     0     0     1     0     0     0     1  ...     0   \n",
      "6039     1     0     0     0     0     0     0     0     0     1  ...     0   \n",
      "\n",
      "      3697  3698  3699  3700  3701  3702  3703  3704  3705  \n",
      "0        0     0     0     0     0     0     0     0     0  \n",
      "1        0     0     0     0     0     0     0     0     0  \n",
      "2        0     0     0     0     0     0     0     0     0  \n",
      "3        0     0     0     0     0     0     0     0     0  \n",
      "4        0     0     0     0     0     0     0     0     0  \n",
      "...    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "6035     0     0     0     0     0     0     0     0     0  \n",
      "6036     0     0     0     0     0     0     0     0     0  \n",
      "6037     0     0     0     0     0     0     0     0     0  \n",
      "6038     0     0     0     0     0     0     0     0     0  \n",
      "6039     0     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[6040 rows x 3706 columns]\n"
     ]
    }
   ],
   "source": [
    "# turn the explicit ratings to implicit feedback data\n",
    "train_mat = (train_mat > 0).astype(float)\n",
    "test_mat = (test_mat > 0).astype(float)\n",
    "\n",
    "# Visualize the new Matrix : rows -> users, columns -> movies\n",
    "print(\"\\n\\nDATA CONVERTED TO IMPLICIT RATINGS\")\n",
    "print(\"--------------------------------------\")\n",
    "print(pd.DataFrame(train_mat, dtype = int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Evaluation metrics*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, we will implement Precision and Recall metrics to evaluate our recommender models. \n",
    "\n",
    "</br>\n",
    "\n",
    "* $ Precision = \\frac{Relevant \\cap Recommended}{Relevant}$\n",
    "\n",
    "* $ Recall = \\frac{Relevant \\cap Recommended}{Recommended}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_n_recall_at_k(recommendations, actual_likes, k):\n",
    "    \n",
    "    if actual_likes.sum() == 0:\n",
    "        return (np.NaN, np.NaN)\n",
    "    \n",
    "    true_positive = 0\n",
    "    for i in range(k):\n",
    "        if actual_likes[int(recommendations[i])] == 1:\n",
    "            true_positive +=1\n",
    "            \n",
    "    precision = true_positive / k\n",
    "    recall = true_positive / actual_likes.sum()\n",
    "    \n",
    "    return (precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(recommended_movies):\n",
    "    \n",
    "    precision_at_5 = []\n",
    "    precision_at_20 = []\n",
    "    precision_at_50 = []\n",
    "\n",
    "    recall_at_5 = []\n",
    "    recall_at_20 = []\n",
    "    recall_at_50 = []\n",
    "\n",
    "    for i in range(test_mat.shape[0]):\n",
    "        # Precision and recall at 5\n",
    "        p_r_5 =  precision_n_recall_at_k(recommendations = recommended_movies[i],\n",
    "                                         actual_likes = test_mat[i], k = 5)\n",
    "        precision_at_5.append(p_r_5[0])\n",
    "        recall_at_5.append(p_r_5[1])\n",
    "\n",
    "        # Precision and recall at 20\n",
    "        p_r_20 =  precision_n_recall_at_k(recommendations = recommended_movies[i],\n",
    "                                         actual_likes = test_mat[i], k = 20)\n",
    "        precision_at_20.append(p_r_20[0])\n",
    "        recall_at_20.append(p_r_20[1])\n",
    "\n",
    "        # Precision and recall at 50\n",
    "        p_r_50 =  precision_n_recall_at_k(recommendations = recommended_movies[i],\n",
    "                                         actual_likes = test_mat[i], k = 50)\n",
    "        precision_at_50.append(p_r_50[0])\n",
    "        recall_at_50.append(p_r_50[1])\n",
    "\n",
    "    print(\"Avg. Precision at 05 : {:.5f}\".format(np.array(precision_at_5).mean()))\n",
    "    print(\"Avg. Precision at 20 : {:.5f}\".format(np.array(precision_at_20).mean()))\n",
    "    print(\"Avg. Precision at 50 : {:.5f}\".format(np.array(precision_at_50).mean()))\n",
    "    print(\"\")\n",
    "    print(\"Avg. Recall at 05 : {:.5f}\".format(np.array(recall_at_5).mean()))\n",
    "    print(\"Avg. Recall at 20 : {:.5f}\".format(np.array(recall_at_20).mean()))\n",
    "    print(\"Avg. Recall at 50 : {:.5f}\".format(np.array(recall_at_50).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Global Average Rating a.k.a. Popular Movie recommender ('Non-personalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvun1WmYL77V"
   },
   "source": [
    "Here we will suggest movies based on their overall popularity, meaning those that have received high ratings from a larger number of users will be recommended to all. Therefore, it is a very naive approach and personalization is not a factor in this approach.\n",
    "</br></br>\n",
    "To determine a movie's popularity, we will calculate its mean rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Popularity based recommender\n",
    "\n",
    "total_implcit_ratings = train_mat.sum(axis = 0)\n",
    "popular_movies = np.argsort(-total_implcit_ratings)\n",
    "recommended_movies = np.zeros((train_mat.shape[0], 50))\n",
    "\n",
    "for user_ind, user_ratings in enumerate(train_mat):\n",
    "    watched_movies = np.nonzero(user_ratings)[0]\n",
    "    counter = 0\n",
    "\n",
    "    recommendation = []\n",
    "    for movie in popular_movies:\n",
    "        if (counter < 50) and (movie not in watched_movies):\n",
    "            recommendation.append(movie)\n",
    "            counter +=1\n",
    "    recommended_movies[user_ind] = recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Precision at 05 : 0.26682\n",
      "Avg. Precision at 20 : 0.20911\n",
      "Avg. Precision at 50 : 0.15792\n",
      "\n",
      "Avg. Recall at 05 : 0.03630\n",
      "Avg. Recall at 20 : 0.10818\n",
      "Avg. Recall at 50 : 0.19471\n"
     ]
    }
   ],
   "source": [
    "evaluate(recommended_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxAu2Gi_fU7m"
   },
   "source": [
    "### 2. User-User Collaborative Filtering\n",
    "\n",
    "User-User Collaborative Filtering is a type of recommendation system that is based on the idea that people who have similar preferences in the past are likely to have similar preferences in the future. This technique uses the behavior and choices of other users to make recommendations to a target user.\n",
    "</br></br>\n",
    "To apply user-user collaborative filtering, the system identifies a target user and then looks for other users with similar tastes and preferences. It then analyzes the ratings and preferences of those similar users to recommend items that the target user has not yet seen or rated. The system can make use of various similarity metrics, such as Pearson correlation or cosine similarity, to identify the most similar users.\n",
    "</br></br>\n",
    "This technique has several advantages over other recommendation approaches, such as content-based filtering. User-user collaborative filtering can identify and recommend items that may be outside a user's typical preferences, but are enjoyed by similar users. It also doesn't rely on metadata or content information about items, which may be incomplete or inaccurate. However, this approach may face scalability issues with larger user bases and may require frequent updates to keep up with changing user preferences.\n",
    "</br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken : 0:00:41.936703\n"
     ]
    }
   ],
   "source": [
    "# Cosine Calculation\n",
    "def cosine(vec1, vec2, user1_ind, user2_ind):  \n",
    "    return np.dot(vec1,vec2) / (user_norm[user1_ind] * user_norm[user2_ind])\n",
    "\n",
    "user_user_similarity = np.zeros((train_mat.shape[0], train_mat.shape[0]))\n",
    "user_norm = np.zeros(train_mat.shape[0])\n",
    "\n",
    "# Calculate L2 norm of each user vector \n",
    "# beforehand to reduce calculation time\n",
    "for ind, row in enumerate(train_mat):\n",
    "    user_norm[ind] = norm(row)\n",
    "    \n",
    "# For each user-user pair calculate cosine similarity\n",
    "start = dt.now()\n",
    "for user1_ind, vec1 in enumerate(train_mat):\n",
    "    user2_ind = user1_ind +1\n",
    "    for vec2 in train_mat[user1_ind+1:, :]:\n",
    "        user_user_similarity[user1_ind][user2_ind] = cosine(vec1, vec2, user1_ind, user2_ind)\n",
    "        user2_ind += 1\n",
    "\n",
    "# Set diagonal cosine values as 0 \n",
    "user_user_similarity = user_user_similarity + user_user_similarity.T\n",
    "end = dt.now()\n",
    "print(\"Time taken : {}\".format((end-start).__str__()))\n",
    "\n",
    "# Identify top N similar users for each user\n",
    "N = 10\n",
    "top_N_similar_users = np.zeros((train_mat.shape[0], 10), dtype = int)\n",
    "for user_ind, user in enumerate(user_user_similarity):\n",
    "    top_N_similar_users[user_ind] = (-user).argsort()[:N]\n",
    "    \n",
    "\n",
    "# Using similarity scores from each user, predict movie ratings for each user\n",
    "cf_recommendations = np.zeros((train_mat.shape[0], train_mat.shape[1]))\n",
    "top_50_recommendations = np.zeros((train_mat.shape[0], 50), dtype = int)\n",
    "\n",
    "for user_ind, user in enumerate(user_user_similarity):\n",
    "    movie_ratings = np.zeros(train_mat.shape[1])\n",
    "    for similar_user in top_N_similar_users[user_ind]:\n",
    "        movie_ratings += user_user_similarity[user_ind,int(similar_user)] * train_mat[similar_user, :]\n",
    "    movie_ratings = movie_ratings / norm([user_user_similarity[i] \n",
    "                                          for i in top_N_similar_users[user_ind]], ord = 1)\n",
    "    cf_recommendations[user_ind] = movie_ratings \n",
    "    \n",
    "# Identify top-50 movies for each user\n",
    "for user_ind in range(train_mat.shape[0]):\n",
    "    watched_movies = np.nonzero(train_mat[user_ind])[0]\n",
    "    recommendation = []\n",
    "    counter = 0\n",
    "    for movie in np.argsort(-cf_recommendations[user_ind]):\n",
    "        if (counter < 50) and (movie not in watched_movies):\n",
    "            recommendation.append(movie)\n",
    "            counter +=1\n",
    "    top_50_recommendations[user_ind] = recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Precision at 05 : 0.37930\n",
      "Avg. Precision at 20 : 0.29185\n",
      "Avg. Precision at 50 : 0.22187\n",
      "\n",
      "Avg. Recall at 05 : 0.07083\n",
      "Avg. Recall at 20 : 0.19116\n",
      "Avg. Recall at 50 : 0.32168\n"
     ]
    }
   ],
   "source": [
    "evaluate(top_50_recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v73XLfN5fU7m"
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Tl2tFlibnAY"
   },
   "source": [
    "### 3. Matrix Factorization\n",
    "\n",
    "Matrix Factorization is a technique used in recommendation systems to model the preferences of users and items in a more efficient way. The idea behind this technique is to factorize a large user-item interaction matrix into two smaller matrices: a user matrix and an item matrix. The user matrix represents the preferences of each user, while the item matrix represents the attributes of each item. </br></br>\n",
    "\n",
    "The factorization process can be achieved using various algorithms, such as Singular Value Decomposition (SVD), Non-Negative Matrix Factorization (NMF), and Alternating Least Squares (ALS). The effectiveness of matrix factorization depends on the quality of the data, the selection of appropriate algorithm and hyperparameters, and the evaluation metrics used to measure the performance of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTLaMD2CbnAS"
   },
   "source": [
    "The MF model can be mathematically represented as: \n",
    "\n",
    "<center>$\\underset{\\mathbf{P},\\mathbf{Q}}{\\text{min}}\\,\\,L=\\sum_{(u,i)\\in\\mathcal{O}}(\\mathbf{P}_u\\cdot\\mathbf{Q}^\\top_i-r_{u,i})^2+\\lambda(\\lVert\\mathbf{P}\\rVert^2_{\\text{F}}+\\lVert\\mathbf{Q}\\rVert^2_{\\text{F}})$,</center>\n",
    "    \n",
    "where $\\mathbf{P}$ is the user latent factor matrix of size (#user, #latent); $\\mathbf{Q}$ is the movie latent factor matrix of size (#movie, #latent); $\\mathcal{O}$ is a user-movie pair set containing all user-movie pairs having ratings in train_mat; $r_{u,i}$ represents the rating for user u and movie i; $\\lambda(\\lVert\\mathbf{P}\\rVert^2_{\\text{F}}+\\lVert\\mathbf{Q}\\rVert^2_{\\text{F}})$ is the regularization term to overcome overfitting problem, $\\lambda$ is the regularization weight (a hyper-parameter manually set by developer, i.e., you), and $\\lVert\\mathbf{P}\\rVert^2_{\\text{F}}=\\sum_{x}\\sum_{y}(\\mathbf{P}_{x,y})^2$, $\\lVert\\mathbf{Q}\\rVert^2_{\\text{F}}=\\sum_{x}\\sum_{y}(\\mathbf{Q}_{x,y})^2$. Such an L function is called the **loss function** for the matrix factorization model. The goal of training an MF model is to find appropriate $\\mathbf{P}$ and $\\mathbf{Q}$ to minimize the loss L."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Po4byk_1bnAZ"
   },
   "source": [
    "The main idea is the same as the MF model for explicit ratings. But instead of predicting explicit ratings, here the MF is to predict binary ratings based on which movies are ranked for users.\n",
    "\n",
    "The challenge now is that in an implicit feedback dataset, there is only positive signal (i.e., '1' in the train_mat) without negative signal. Hence, to let our MF work for this implicit feedback, a simple but powerful method -- random negative sampling -- is adopted. The main idea is that in each training epoch, we randomly sample user-movie pairs without positive feedback in train_mat (user-movie pairs with '0' in train_mat) to be negative feedback, and mix them with positive feedback as the training data to train the MF model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "NCQi55FTbnAa"
   },
   "outputs": [],
   "source": [
    "class MF_implicit:\n",
    "    def __init__(self, train_mat, test_mat, latent=5, lr=0.01, reg=0.01):\n",
    "        self.train_mat = train_mat  # the training rating matrix of size (#user, #movie)\n",
    "        self.test_mat = test_mat  # the training rating matrix of size (#user, #movie)\n",
    "        \n",
    "        self.latent = latent  # the latent dimension\n",
    "        self.lr = lr  # learning rate\n",
    "        self.reg = reg  # regularization weight, i.e., the lambda in the objective function\n",
    "        \n",
    "        self.num_user, self.num_movie = train_mat.shape\n",
    "        \n",
    "        self.sample_user, self.sample_movie = self.train_mat.nonzero()  # get the user-movie paris having ratings in train_mat\n",
    "        self.num_sample = len(self.sample_user)  # the number of user-movie pairs having ratings in train_mat\n",
    "\n",
    "        self.user_test_like = []\n",
    "        for u in range(self.num_user):\n",
    "            self.user_test_like.append(np.where(self.test_mat[u, :] > 0)[0])\n",
    "\n",
    "        self.P = np.random.random((self.num_user, self.latent))  # latent factors for users, size (#user, self.latent), randomly initialized\n",
    "        self.Q = np.random.random((self.num_movie, self.latent))  # latent factors for users, size (#movie, self.latent), randomly initialized\n",
    "        \n",
    "    def negative_sampling(self):\n",
    "        negative_movie = np.random.choice(np.arange(self.num_movie), size=(len(self.sample_user)), replace=True)\n",
    "        true_negative = self.train_mat[self.sample_user, negative_movie] == 0\n",
    "        negative_user = self.sample_user[true_negative]\n",
    "        negative_movie = negative_movie[true_negative]\n",
    "        return np.concatenate([self.sample_user, negative_user]), np.concatenate([self.sample_movie, negative_movie])\n",
    "\n",
    "    def train(self, epoch=20):\n",
    "\n",
    "        \n",
    "        for ep in range(epoch):\n",
    "           \n",
    "            # Call negative_sampling to get data\n",
    "            users, movies = self.negative_sampling()\n",
    "            \n",
    "            # Shuffle the data\n",
    "            p = np.random.permutation(len(users))\n",
    "            users, movies = users[p], movies[p]\n",
    "            \n",
    "            # Iterate over each user-movie pair and train\n",
    "            for user, movie in zip(users, movies):\n",
    "                r_ui = self.train_mat[user, movie]\n",
    "                self.P[user, :] -= self.lr * (2* (np.dot(self.P[user, :],self.Q[movie, :]) - r_ui) *\n",
    "                                              self.Q[movie, :] + 2 * self.reg * self.P[user, :] )\n",
    "                self.Q[movie, :] -= self.lr * (2* (np.dot(self.P[user, :],self.Q[movie, :]) - r_ui) *\n",
    "                                              self.P[user, :] + 2 * self.reg * self.Q[movie, :] )\n",
    "                               \n",
    "\n",
    "        # Test the model\n",
    "        self.test()\n",
    "        \n",
    "            \n",
    "    def predict(self):\n",
    "\n",
    "        predictions = np.matmul(self.P,  self.Q.T )\n",
    "        recommendation = np.zeros((self.train_mat.shape[0], 50), dtype = int)\n",
    "        \n",
    "        for user_ind in range(self.train_mat.shape[0]):\n",
    "            watched_movies = np.nonzero(self.train_mat[user_ind])[0]\n",
    "            recommendation_user = []\n",
    "            counter = 0\n",
    "            for movie in np.argsort(-predictions[user_ind]):\n",
    "                if (counter < 50) and (movie not in watched_movies):\n",
    "                    recommendation_user.append(movie)\n",
    "                    counter +=1\n",
    "            recommendation[user_ind] = recommendation_user\n",
    "        \n",
    "        return recommendation\n",
    "    \n",
    "    def test(self):\n",
    "        recommendation = self.predict()\n",
    "\n",
    "        recalls = np.zeros(3)\n",
    "        precisions = np.zeros(3)\n",
    "        user_count = 0.\n",
    "\n",
    "        for u in range(self.num_user):\n",
    "            test_like = self.user_test_like[u]\n",
    "            test_like_num = len(test_like)\n",
    "            if test_like_num == 0:\n",
    "                continue\n",
    "            rec = recommendation[u, :]\n",
    "            hits = np.zeros(3)\n",
    "            for k in range(50):\n",
    "                if rec[k] in test_like:\n",
    "                    if k < 50:\n",
    "                        hits[2] += 1\n",
    "                        if k < 20:\n",
    "                            hits[1] += 1\n",
    "                            if k < 5:\n",
    "                                hits[0] += 1\n",
    "            recalls[0] += (hits[0] / test_like_num)\n",
    "            recalls[1] += (hits[1] / test_like_num)\n",
    "            recalls[2] += (hits[2] / test_like_num)\n",
    "            precisions[0] += (hits[0] / 5.)\n",
    "            precisions[1] += (hits[1] / 20.)\n",
    "            precisions[2] += (hits[2] / 50.)\n",
    "            user_count += 1\n",
    "\n",
    "        recalls /= user_count\n",
    "        precisions /= user_count\n",
    "\n",
    "        print('recall@5\\t[%.6f],\\t||\\t recall@20\\t[%.6f],\\t||\\t recall@50\\t[%.6f]' % (recalls[0], recalls[1], recalls[2]))\n",
    "        print('precision@5\\t[%.6f],\\t||\\t precision@20\\t[%.6f],\\t||\\t precision@50\\t[%.6f]' % (precisions[0], precisions[1], precisions[2]))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GK9lahvXbnAa"
   },
   "source": [
    "Now, run the next cell to build and train your implemented MF model for implicit feedback. The expected time used for training one epoch is 20s to 2 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "l1K6sLgEbnAa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@5\t[0.039198],\t||\t recall@20\t[0.123433],\t||\t recall@50\t[0.238562]\n",
      "precision@5\t[0.293212],\t||\t precision@20\t[0.243071],\t||\t precision@50\t[0.197904]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mf_implicit = MF_implicit(train_mat, test_mat, latent=5, lr=0.01, reg=0.0001)\n",
    "mf_implicit.train(epoch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxwdsGYel961"
   },
   "source": [
    "Although, the performance of Matrix Factorization is slightly better than non-personalized recommender system, Matrix Factorization (MF) is not providing a better result that User-user collaborative filtering (UUCF). <br>\n",
    "\n",
    "MF tries to find the characteristics of Users and Movies through its latent factors and hence it has a personalized touch to it. So, it is better than a non-personlaized recommeder system.  <br>\n",
    "\n",
    "The poor performance of MF may be because of the implicit nature of dataset. Here we are giving incorrect information to the model by saying that a user movie pair with no feedback implies it is a negative feedback. Hence, the latent features learned by the model can be inaccurate leading to poor performance.\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PW9KO1gMD-e"
   },
   "source": [
    "### 4. Bayesian Personalized Ranking (BPR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MO69amdAMD-k"
   },
   "source": [
    "The BPR model can be mathematically represented as: \n",
    "\n",
    "<center>$\\underset{\\mathbf{P},\\mathbf{Q}}{\\text{max}}\\,\\,L=\\sum_{(u,i,j)\\in\\mathcal{O}}\\sigma(\\mathbf{P}_u\\cdot\\mathbf{Q}^\\top_i-\\mathbf{P}_u\\cdot\\mathbf{Q}^\\top_j)+\\lambda(\\lVert\\mathbf{P}\\rVert^2_{\\text{F}}+\\lVert\\mathbf{Q}\\rVert^2_{\\text{F}})$,</center>\n",
    "    \n",
    "where $\\mathbf{P}$ is the user latent factor matrix of size (#user, #latent); $\\mathbf{Q}$ is the movie latent factor matrix of size (#movie, #latent); $\\sigma(\\cdot)$ is the Sigmoid function; $\\mathcal{O}$ is a (user, positive movie, negative movie) tuple set, and each tuple $(u,i,j)$ is a training sample with user $u$ and a posotive movie $i$ with value 1 in train_mat and a negative movie $j$ with value 0 in train_mat; $\\lambda(\\lVert\\mathbf{P}\\rVert^2_{\\text{F}}+\\lVert\\mathbf{Q}\\rVert^2_{\\text{F}})$ is the regularization term to overcome overfitting problem, $\\lambda$ is the regularization weight (a hyper-parameter manually set by the developer, i.e., you), and $\\lVert\\mathbf{P}\\rVert^2_{\\text{F}}=\\sum_{x}\\sum_{y}(\\mathbf{P}_{x,y})^2$, $\\lVert\\mathbf{Q}\\rVert^2_{\\text{F}}=\\sum_{x}\\sum_{y}(\\mathbf{Q}_{x,y})^2$. Such an L function is called the **loss function** for the matrix factorization model. The goal of training a BPR model is to find appropriate $\\mathbf{P}$ and $\\mathbf{Q}$ to **maximize** the loss L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "R7oKJUR9MD-l"
   },
   "outputs": [],
   "source": [
    "class BPR:\n",
    "    def __init__(self, train_mat, test_mat, latent=5, lr=0.01, reg=0.01):\n",
    "        self.train_mat = train_mat  # the training rating matrix of size (#user, #movie)\n",
    "        self.test_mat = test_mat  # the training rating matrix of size (#user, #movie)\n",
    "        \n",
    "        self.latent = latent  # the latent dimension\n",
    "        self.lr = lr  # learning rate\n",
    "        self.reg = reg  # regularization weight, i.e., the lambda in the objective function\n",
    "        \n",
    "        self.num_user, self.num_movie = train_mat.shape\n",
    "        \n",
    "        self.positive_user, self.positive_movie = self.train_mat.nonzero()  # get the user-movie paris having ratings in train_mat\n",
    "\n",
    "        self.user_test_like = []\n",
    "        for u in range(self.num_user):\n",
    "            self.user_test_like.append(np.where(self.test_mat[u, :] > 0)[0])\n",
    "\n",
    "        self.P = np.random.random((self.num_user, self.latent))  # latent factors for users, size (#user, self.latent), randomly initialized\n",
    "        self.Q = np.random.random((self.num_movie, self.latent))  # latent factors for users, size (#movie, self.latent), randomly initialized\n",
    "        \n",
    "    def negative_sampling(self): \n",
    "        # do the negative sampling for each of the positive user-movie pair. Here we set negative sampling rate as 2, \n",
    "        # i.e., for each positive user-movie pair, randomly sample two negative movies. \n",
    "        # This function returns final training data: a user list, a positive movie list, and a negative movie list\n",
    "        sample_user = np.tile(self.positive_user, 2)\n",
    "        sample_pos_movie = np.tile(self.positive_movie, 2)\n",
    "        sample_neg_movie = np.random.choice(np.arange(self.num_movie), size=(len(sample_user)), replace=True)\n",
    "        true_negative = self.train_mat[sample_user, sample_neg_movie] == 0\n",
    "        return sample_user[true_negative], sample_pos_movie[true_negative], sample_neg_movie[true_negative]\n",
    "\n",
    "    def train(self, epoch=20):\n",
    "\n",
    "        for ep in range(epoch):\n",
    "\n",
    "            \n",
    "            # Call negative_sampling to get data\n",
    "            users, pos_movies, neg_movies = self.negative_sampling()\n",
    "            \n",
    "            # Shuffle the data\n",
    "            p = np.random.permutation(len(users))\n",
    "            users, pos_movies, neg_movies = users[p], pos_movies[p], neg_movies[p] \n",
    "            \n",
    "            # Iterate over each user-movie pair and train\n",
    "            for user, pos_movie, neg_movie in zip(users, pos_movies, neg_movies):\n",
    "                x_hat = np.matmul(self.P[user, :], self.Q[pos_movie, :]) - \\\n",
    "                        np.matmul(self.P[user, :], self.Q[neg_movie, :])\n",
    "                self.P[user, :] -= self.lr * ( - np.exp(-x_hat)/(1 +np.exp(-x_hat)) * \n",
    "                                              (self.Q[pos_movie, :] - self.Q[neg_movie, :]) +\n",
    "                                              self.reg * self.P[user, :])\n",
    "                self.Q[pos_movie, :] -= self.lr * ( - np.exp(-x_hat)/(1 +np.exp(-x_hat)) * \n",
    "                                              self.P[user, :] + self.reg * self.Q[pos_movie, :])\n",
    "                self.Q[neg_movie, :] -= self.lr * (np.exp(-x_hat)/(1 +np.exp(-x_hat)) * \n",
    "                                              self.P[user, :] + self.reg * self.Q[neg_movie, :])\n",
    "                  \n",
    "            \n",
    "        # Test the model\n",
    "        self.test()\n",
    "\n",
    "            \n",
    "    def predict(self):\n",
    "        # The prediction function, which generates the ranked lists of movies \n",
    "        # by the trained BPR for every user, store the result (named 'recommendation') in a numpy array of size (#user, 50), where entry (u, k) \n",
    "        # represents the movie id that is ranked at position k in the recommendation list to user u. Return the 'recommendation' variable. \n",
    "        prediction_mat = np.matmul(self.P, self.Q.T)\n",
    "        recommendation = []\n",
    "        for u in range(self.num_user):\n",
    "            scores = prediction_mat[u]\n",
    "            train_like = np.where(train_mat[u, :] > 0)[0]\n",
    "            scores[train_like] = -9999\n",
    "            top50_iid = np.argpartition(scores, -50)[-50:]\n",
    "            top50_iid = top50_iid[np.argsort(scores[top50_iid])[-1::-1]]\n",
    "            recommendation.append(top50_iid)\n",
    "        recommendation = np.array(recommendation)\n",
    "        return recommendation\n",
    "    \n",
    "    def test(self):\n",
    "        recommendation = self.predict()\n",
    "\n",
    "        recalls = np.zeros(3)\n",
    "        precisions = np.zeros(3)\n",
    "        user_count = 0.\n",
    "\n",
    "        for u in range(self.num_user):\n",
    "            test_like = self.user_test_like[u]\n",
    "            test_like_num = len(test_like)\n",
    "            if test_like_num == 0:\n",
    "                continue\n",
    "            rec = recommendation[u, :]\n",
    "            hits = np.zeros(3)\n",
    "            for k in range(50):\n",
    "                if rec[k] in test_like:\n",
    "                    if k < 50:\n",
    "                        hits[2] += 1\n",
    "                        if k < 20:\n",
    "                            hits[1] += 1\n",
    "                            if k < 5:\n",
    "                                hits[0] += 1\n",
    "            recalls[0] += (hits[0] / test_like_num)\n",
    "            recalls[1] += (hits[1] / test_like_num)\n",
    "            recalls[2] += (hits[2] / test_like_num)\n",
    "            precisions[0] += (hits[0] / 5.)\n",
    "            precisions[1] += (hits[1] / 20.)\n",
    "            precisions[2] += (hits[2] / 50.)\n",
    "            user_count += 1\n",
    "\n",
    "        recalls /= user_count\n",
    "        precisions /= user_count\n",
    "\n",
    "        print('recall@5\\t[%.6f],\\t||\\t recall@20\\t[%.6f],\\t||\\t recall@50\\t[%.6f]' % (recalls[0], recalls[1], recalls[2]))\n",
    "        print('precision@5\\t[%.6f],\\t||\\t precision@20\\t[%.6f],\\t||\\t precision@50\\t[%.6f]' % (precisions[0], precisions[1], precisions[2]))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "T-q9CtdDMD-s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall@5\t[0.050495],\t||\t recall@20\t[0.141984],\t||\t recall@50\t[0.261247]\n",
      "precision@5\t[0.361854],\t||\t precision@20\t[0.277268],\t||\t precision@50\t[0.215030]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bpr = BPR(train_mat, test_mat, latent=5, lr=0.01, reg=0.001)\n",
    "bpr.train(epoch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0M28QNVMD-s"
   },
   "source": [
    "BPR is providing much better result than non-personalized recommender system and Matrix factorization (MF) and a similar result as that of User-user collaborative filtering (UUCF). <br>\n",
    "\n",
    "It performs better than non-personalized recommenders as it is a personalized recommender system that can capture user preferences and movie characteristics. BPR is expected to perform better than MF as BPR is designed for implicit ratings (where an absence of rating doesn't mean its negatively rated). Moreover, BPR directly optimizes the ranking of items, which is a more natural and intuitive objective for recommendation systems than predicting explicit ratings. \n",
    "\n",
    "------\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## THE END"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
